{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pythonで学ぶ強化学習[改訂第2版]　入門から実践まで　著:久保隆宏"
      ],
      "metadata": {
        "id": "smq-jxP9EvjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code1"
      ],
      "metadata": {
        "id": "JjNAvlbxT4OP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ライブラリのインストール"
      ],
      "metadata": {
        "id": "_oZsJoJyEfO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from enum import Enum"
      ],
      "metadata": {
        "id": "H_bY0qmVVSHs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. クラス State\n",
        "\n",
        "目的: グリッドワールド内のエージェントの位置を表現するためのクラスです。行（row）と列（column）のインデックスを保持します。\n",
        "\n",
        "メソッド:\n",
        "\n",
        "__init__: 初期状態を設定します。デフォルトでは行と列は -1 に設定されていますが、通常は有効なグリッド位置が設定されます。\n",
        "\n",
        "__repr__: 状態の文字列表現を返します。デバッグやログ出力時に便利です。\n",
        "\n",
        "**clone**: 現在の状態のコピーを作成します。状態の変更が他の参照に影響しないようにします。\n",
        "\n",
        "__hash__ と __eq__: 状態をハッシュ可能にし、集合や辞書のキーとして使用可能にします。また、状態同士の比較を可能にします。"
      ],
      "metadata": {
        "id": "eXC7iUC8VVUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 状態を表すクラス\n",
        "class State:\n",
        "    def __init__(self, row=-1, column=-1):\n",
        "        self.row = row        # 行番号\n",
        "        self.column = column  # 列番号\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<State: [{self.row}, {self.column}]>\"\n",
        "\n",
        "    def clone(self):\n",
        "        return State(self.row, self.column)\n",
        "\n",
        "    def __hash__(self):\n",
        "        # 状態をハッシュ化可能にするために必要\n",
        "        return hash((self.row, self.column))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        # 状態同士の比較を可能にする\n",
        "        return self.row == other.row and self.column == other.column\n"
      ],
      "metadata": {
        "id": "ykffwgzbVV6x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 列挙型 Action\n",
        "目的: エージェントが取れる行動（UP, DOWN, LEFT, RIGHT）を定義します。各行動にはユニークな値が割り当てられています。\n",
        "\n",
        "値:\n",
        "\n",
        "UP = 1, DOWN = -1, LEFT = 2, RIGHT = -2"
      ],
      "metadata": {
        "id": "exfpHpg9WSS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 行動を定義する列挙型\n",
        "class Action(Enum):\n",
        "    UP = 1\n",
        "    DOWN = -1\n",
        "    LEFT = 2\n",
        "    RIGHT = -2"
      ],
      "metadata": {
        "id": "HOyyJJjlWlvf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. クラス Environment\n",
        "目的: グリッドワールドの環境を管理します。エージェントの位置、グリッドの状態、行動の遷移、報酬の計算などを担当します。\n",
        "\n",
        "属性:\n",
        "\n",
        "grid: グリッドの2次元リスト(形や障害物はmainで指定)。各セルの値はそのセルの属性を示します。\n",
        "\n",
        "0: 通常のセル\n",
        "\n",
        "1: ゴールセル（報酬 +1）\n",
        "\n",
        "-1: 罰則セル（報酬 -1）\n",
        "\n",
        "9: 障害物（エージェントは通過できません）\n",
        "\n",
        "agent_state: エージェントの現在の状態（Stateオブジェクト）。\n",
        "\n",
        "default_reward: 通常の移動に対するデフォルトの報酬（-0.04）。\n",
        "\n",
        "move_prob: 指定した行動が成功する確率（デフォルトは0.8）。"
      ],
      "metadata": {
        "id": "XNWslipQ-DDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 環境を表すクラス\n",
        "class Environment:\n",
        "    def __init__(self, grid, move_prob=0.8):\n",
        "        self.grid = grid                    # 環境のグリッド（2Dリスト）\n",
        "        self.agent_state = State()          # エージェントの現在の状態\n",
        "        self.default_reward = -0.04        # デフォルトの報酬\n",
        "        self.move_prob = move_prob          # 行動が成功する確率\n",
        "        self.reset()                        # 環境の初期化\n",
        "\n",
        "    \"\"\"\n",
        "    プロパティ:\n",
        "    row_length: グリッドの行数。\n",
        "    column_length: グリッドの列数。\n",
        "    actions: 可能な行動のリスト（Action列挙型のリスト）。\n",
        "    states: グリッド上の有効なすべての状態（障害物を除く）。\n",
        "    \"\"\"\n",
        "    @property\n",
        "    def row_length(self):\n",
        "        return len(self.grid)  # グリッドの行数\n",
        "\n",
        "    @property\n",
        "    def column_length(self):\n",
        "        return len(self.grid[0])  # グリッドの列数\n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "        return [Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT]  # 可能な行動のリスト\n",
        "\n",
        "    @property\n",
        "    def states(self):\n",
        "        # グリッド上の有効なすべての状態を取得\n",
        "        states = []\n",
        "        for row in range(self.row_length):\n",
        "            for column in range(self.column_length):\n",
        "                if self.grid[row][column] != 9:  # 9は障害物を示す\n",
        "                    states.append(State(row, column))\n",
        "        return states\n",
        "\n",
        "    \"\"\"\n",
        "    メソッド:\n",
        "    \"\"\"\n",
        "    def transit_func(self, state, action):\n",
        "        \"\"\"\n",
        "        現在の状態と行動に基づいて、可能な遷移先の状態とその確率を計算します。行動が成功する確率（move_prob）と失敗した場合に他の方向に動く確率を考慮します。\n",
        "        \"\"\"\n",
        "        transition_probs = {}\n",
        "        if not self.can_action_at(state):\n",
        "            return transition_probs  # 行動できない場合は空の辞書を返す\n",
        "\n",
        "        try:\n",
        "            opposite_direction = Action(action.value * -1)  # 逆方向の行動を取得\n",
        "        except ValueError:\n",
        "            opposite_direction = None  # 逆方向が定義されていない場合\n",
        "\n",
        "        for a in self.actions:\n",
        "            prob = 0\n",
        "            if a == action:\n",
        "                prob = self.move_prob  # 指定された行動の成功確率\n",
        "            elif opposite_direction and a != opposite_direction:\n",
        "                prob = (1 - self.move_prob) / 2  # 他の行動の確率\n",
        "            else:\n",
        "                continue  # 逆方向の行動は確率に含めない\n",
        "\n",
        "            next_state = self._move(state, a)  # 次の状態を計算\n",
        "            if next_state not in transition_probs:\n",
        "                transition_probs[next_state] = prob\n",
        "            else:\n",
        "                transition_probs[next_state] += prob  # 確率を累積\n",
        "\n",
        "        return transition_probs\n",
        "\n",
        "    def can_action_at(self, state):\n",
        "        \"\"\"\n",
        "        指定された状態で行動が可能かどうかを判定します。\n",
        "        0は通常のセル、9は障害物を示す。\n",
        "        \"\"\"\n",
        "        return self.grid[state.row][state.column] == 0\n",
        "\n",
        "    def _move(self, state, action):\n",
        "        \"\"\"\n",
        "        指定された状態と行動に基づいて次の状態を計算します。\n",
        "        \"\"\"\n",
        "        if not self.can_action_at(state):\n",
        "            raise Exception(\"Can't move from here!\")\n",
        "\n",
        "        next_state = state.clone()\n",
        "\n",
        "        # 行動に応じて状態を更新\n",
        "        if action == Action.UP:\n",
        "            next_state.row -= 1  # 上に移動\n",
        "        elif action == Action.DOWN:\n",
        "            next_state.row += 1  # 下に移動\n",
        "        elif action == Action.LEFT:\n",
        "            next_state.column -= 1  # 左に移動\n",
        "        elif action == Action.RIGHT:\n",
        "            next_state.column += 1  # 右に移動\n",
        "\n",
        "        # グリッドの境界チェック\n",
        "        if not (0 <= next_state.row < self.row_length):\n",
        "            next_state = state  # 境界外なら移動しない\n",
        "        if not (0 <= next_state.column < self.column_length):\n",
        "            next_state = state  # 境界外なら移動しない\n",
        "        # 障害物チェック\n",
        "        if self.grid[next_state.row][next_state.column] == 9:\n",
        "            next_state = state  # 障害物があれば移動しない\n",
        "\n",
        "        return next_state\n",
        "\n",
        "    def reward_func(self, state):\n",
        "        \"\"\"\n",
        "        指定された状態に対する報酬とエピソードの終了判定を返します。\n",
        "        \"\"\"\n",
        "        reward = self.default_reward  # デフォルトの報酬\n",
        "        done = False\n",
        "\n",
        "        attribute = self.grid[state.row][state.column]\n",
        "        if attribute == 1:\n",
        "            reward = 1   # ゴールへの報酬\n",
        "            done = True  # エピソード終了\n",
        "        elif attribute == -1:\n",
        "            reward = -1  # 罰則の報酬\n",
        "            done = True  # エピソード終了\n",
        "\n",
        "        return reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        環境を初期状態にリセットします。\n",
        "        エージェントの位置をグリッドの左下に設定。\n",
        "        \"\"\"\n",
        "        self.agent_state = State(self.row_length - 1, 0)\n",
        "        return self.agent_state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        エージェントを一歩進め、次の状態、報酬、終了フラグを返します。\n",
        "        \"\"\"\n",
        "        next_state, reward, done = self.transit(self.agent_state, action)\n",
        "        if next_state is not None:\n",
        "            self.agent_state = next_state\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def transit(self, state, action):\n",
        "        \"\"\"\n",
        "        遷移確率に基づいて次の状態をサンプリングし、報酬と終了フラグを返します。\n",
        "        \"\"\"\n",
        "        transition_probs = self.transit_func(state, action)\n",
        "        if len(transition_probs) == 0:\n",
        "            return None, None, True  # 行動できない場合はエピソード終了\n",
        "\n",
        "        next_states = []\n",
        "        probs = []\n",
        "        for s in transition_probs:\n",
        "            next_states.append(s)\n",
        "            probs.append(transition_probs[s])\n",
        "\n",
        "        # 確率に基づいて次の状態を選択\n",
        "        next_state = np.random.choice(next_states, p=probs)\n",
        "        reward, done = self.reward_func(next_state)\n",
        "        return next_state, reward, done\n"
      ],
      "metadata": {
        "id": "q0Z7kHlP-2uu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. クラス Agent\n",
        "目的: 環境内で行動を選択するエージェントを表します。ここではランダムな行動選択ポリシーを実装しています。\n",
        "\n",
        "属性:\n",
        "\n",
        "actions: 環境で可能な行動のリスト。\n",
        "メソッド:\n",
        "\n",
        "policy: 現在の状態に基づいて行動を選択します。ここではランダムに行動を選択していますが、将来的にはより複雑なポリシー（例えば、価値反復法やQ学習）を実装することができます。"
      ],
      "metadata": {
        "id": "_0Ca9T0YCEPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# エージェントを表すクラス\n",
        "class Agent:\n",
        "    def __init__(self, env):\n",
        "        self.actions = env.actions  # 環境で可能な行動のリスト\n",
        "\n",
        "    def policy(self, state):\n",
        "        \"\"\"\n",
        "        現在の状態に基づいて行動を選択します。\n",
        "        ここではランダムに行動を選択しています。\n",
        "        \"\"\"\n",
        "        return np.random.choice(self.actions)\n"
      ],
      "metadata": {
        "id": "3iL9rU6CCDkD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 関数 main\n",
        "目的: 環境とエージェントを初期化し、指定されたエピソード数だけシミュレーションを実行します。各エピソードの終了時にエージェントの総報酬を出力します。\n",
        "\n",
        "手順:\n",
        "\n",
        "1.グリッドの定義:\n",
        "\n",
        "*   3x4のグリッドを定義しています。\n",
        "*   1 はゴールセル、-1 は罰則セル、9 は障害物を表します。\n",
        "*   例えば、grid[0][3] = 1 は最上行の右端がゴールセルであることを示します。\n",
        "\n",
        "2.環境とエージェントの初期化:\n",
        "  \n",
        "\n",
        "*   Environment オブジェクトを作成し、グリッドを渡します。\n",
        "*   Agent オブジェクトを作成し、環境を渡します。\n",
        "\n",
        "3.エピソードの実行:\n",
        "\n",
        "\n",
        "*   指定されたエピソード数（ここでは10）だけループします。\n",
        "*   各エピソードで環境をリセットし、エージェントの総報酬を初期化します。\n",
        "*   エピソードが終了するまでループし、エージェントが選択した行動を環境に適用します。\n",
        "*   エピソードの終了時に総報酬を出力します。\n",
        "*   無限ループ防止: ステップ数が100を超えた場合、エピソードを強制終了します。"
      ],
      "metadata": {
        "id": "5gHUWSJNCVwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# メイン関数\n",
        "def main():\n",
        "    # グリッドの定義\n",
        "    # 0: 通常のセル\n",
        "    # 1: ゴール（報酬 +1）\n",
        "    # -1: 罰則セル（報酬 -1）\n",
        "    # 9: 障害物\n",
        "    grid = [\n",
        "        [0, 0, 0, 1],\n",
        "        [0, 9, 0, -1],\n",
        "        [0, 0, 0, 0]\n",
        "    ]\n",
        "\n",
        "    env = Environment(grid)  # 環境の初期化\n",
        "    agent = Agent(env)       # エージェントの初期化\n",
        "\n",
        "    num_episodes = 10  # エピソード数\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        state = env.reset()          # 環境のリセット\n",
        "        total_reward = 0            # 総報酬の初期化\n",
        "        done = False                # エピソード終了フラグ\n",
        "        step_count = 0              # ステップ数のカウンタ（オプション）\n",
        "\n",
        "        while not done:\n",
        "            action = agent.policy(state)          # 行動の選択\n",
        "            next_state, reward, done = env.step(action)  # 環境の更新\n",
        "            total_reward += reward                # 報酬の累積\n",
        "            state = next_state                    # 状態の更新\n",
        "            step_count += 1\n",
        "\n",
        "            # ステップ数が一定を超えたらエピソードを終了（無限ループ防止）\n",
        "            if step_count > 100:\n",
        "                print(f\"Episode {i}: Exceeded step limit.\")\n",
        "                break\n",
        "\n",
        "        print(f\"Episode {i}: Agent Score: {total_reward}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vkQZGdkAStGP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.スクリプトの実行\n",
        "\n",
        "if __name__ == \"__main__\": の条件下で main() 関数が呼び出され、スクリプトが実行されます。これにより、他のモジュールからインポートされた場合に main() が自動的に実行されないようになります。"
      ],
      "metadata": {
        "id": "MyHaPiupDdgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# スクリプトとして実行された場合にmain()を呼び出す\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ladd38dBDiHk",
        "outputId": "3e36683b-dfbd-4565-fd59-1da1adeecc3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Agent Score: -1.3599999999999999\n",
            "Episode 1: Agent Score: -0.4000000000000006\n",
            "Episode 2: Agent Score: -0.6400000000000008\n",
            "Episode 3: Exceeded step limit.\n",
            "Episode 3: Agent Score: -4.040000000000003\n",
            "Episode 4: Agent Score: 0.52\n",
            "Episode 5: Agent Score: -3.000000000000001\n",
            "Episode 6: Agent Score: 0.31999999999999995\n",
            "Episode 7: Agent Score: -1.9200000000000004\n",
            "Episode 8: Agent Score: -3.4400000000000013\n",
            "Episode 9: Agent Score: -1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code2"
      ],
      "metadata": {
        "id": "z78JxdpiEo_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### bellman_equation"
      ],
      "metadata": {
        "id": "eXtT0STzEs9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.価値関数V(s)\n",
        "\n",
        "目的: 状態 s における価値関数 V(s) を計算します。\n",
        "\n",
        "動作:\n",
        "* 現在の状態 s における即時報酬 R(s) を取得します。\n",
        "* 割引率 gamma を用いて、次の状態における最大の価値 max_V_on_next_state(s) を計算し、これを即時報酬に加算します。\n",
        "\n",
        "割引率 gamma: 将来の報酬に対する現在の価値の重要度を決定します。ここでは gamma=0.99 と設定されており、未来の報酬も高く評価されます。"
      ],
      "metadata": {
        "id": "5bL_c6uTHasP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def V(s, gamma=0.99):\n",
        "  V = R(s) + gamma * max_V_on_next_state(s)\n",
        "  return V"
      ],
      "metadata": {
        "id": "XEvBu5MRHggS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.即時報酬関数R(s)\n",
        "\n",
        "目的: 状態 s に対する即時報酬を返します。\n",
        "\n",
        "動作:\n",
        "*  状態が \"happy_end\" の場合、報酬 1 を返します。\n",
        "*  状態が \"bad_end\" の場合、報酬 -1 を返します。\n",
        "*  その他の状態では報酬 0 を返します。\n",
        "\n",
        "用途: エージェントが特定の状態に到達した際の報酬を定義します。"
      ],
      "metadata": {
        "id": "0YwxPkuYHhnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def R(s):\n",
        "  if s == \"happy_end\":\n",
        "    return 1\n",
        "  elif s == \"bad_end\":\n",
        "    return -1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "vpocRbtBHhM1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 最大次状態価値関数 max_V_on_next_state(s)\n",
        "目的: 現在の状態 s から遷移可能な次の状態の中で、最大の価値 V を持つものを見つけ、その値を返します。\n",
        "\n",
        "動作:\n",
        "* もし状態 s が \"happy_end\" または \"bad_end\" であれば、次の価値は 0 とします（終端状態のため）。\n",
        "* それ以外の場合、可能な行動（\"up\", \"down\"）に対して以下を実行:\n",
        "  * 行動 a を取った場合の遷移確率を transit_func(s, a) で取得。\n",
        "  * 各遷移先 next_state とその確率 prob に対して、V(next_state) を計算し、確率で重み付けして合計します。\n",
        "  * 各行動に対する総合価値 v を values リストに追加。\n",
        "* values が空でない場合、リスト内の最大値を返します。空の場合は 0 を返します。"
      ],
      "metadata": {
        "id": "byp9ASBkIcX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_V_on_next_state(s):\n",
        "  if s in [\"happy_end\", \"bad_end\"]:\n",
        "    return 0\n",
        "\n",
        "  actions = [\"up\", \"down\"]\n",
        "  values = []\n",
        "  for a in actions:\n",
        "    transition_probs = transit_func(s,a)\n",
        "    v = 0\n",
        "    for next_state in transition_probs:\n",
        "      prob = transition_probs[next_state]\n",
        "      v += prob * V(next_state)\n",
        "    values.append(v)\n",
        "  return max(values)"
      ],
      "metadata": {
        "id": "qoWMpJlSEsAN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 遷移関数 transit_func(s, a)\n",
        "目的: 現在の状態 s と行動 a に基づいて、次の状態とその遷移確率を返します。\n",
        "\n",
        "動作:\n",
        "* 行動履歴の取得:\n",
        "  * 状態 s は \"state_up_down_up\" のような文字列で表現されており、split(\"_\")[1:] によって行動履歴（例: [\"up\", \"down\", \"up\"]）を取得。\n",
        "* ゲームステップ数のチェック:\n",
        "  * 定数 LIMIT_GAME_COUNT = 5 に基づき、行動履歴の長さが5に達しているかを確認。\n",
        "  * もし5に達している場合:\n",
        "    * 行動履歴内の \"up\" の回数をカウント。\n",
        "    * \"up\" の回数が HAPPY_END_BOTDER = 4 以上であれば \"happy_end\"、そうでなければ \"bad_end\" に遷移。\n",
        "    * 遷移確率は 1.0（確定的）。\n",
        "* ゲームステップ数が制限に達していない場合:\n",
        "  * 行動 a が \"up\" の場合、逆行動は \"down\"、逆に \"down\" の場合は \"up\" と設定。\n",
        "  * 行動 a が成功する確率 MOVE_PROB = 0.9 で、逆行動が発生する確率は 1 - MOVE_PROB = 0.1 として遷移先を設定。\n",
        "  * 具体的には、次の状態は \"state_up\"（行動 a が \"up\" の場合）で 0.9 の確率で遷移し、逆行動 \"down\" の場合は \"state_down\" に 0.1 の確率で遷移します。\n",
        "* 内部関数 next_state(state, action):\n",
        "  * 現在の状態 state に行動 action を適用した次の状態を生成します。例: \"state_up\"。"
      ],
      "metadata": {
        "id": "AEmVCLYeJS5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transit_func(s, a):\n",
        "  actions = s.split(\"_\")[1:]\n",
        "  LIMIT_GAME_COUNT = 5\n",
        "  HAPPY_END_BOTDER = 4\n",
        "  MOVE_PROB = 0.9\n",
        "\n",
        "  def next_state(state, action):\n",
        "    return \"_\".join([state, action])\n",
        "\n",
        "  if len(actions) == LIMIT_GAME_COUNT:\n",
        "    up_count = sum([1 if a == \"up\" else 0 for a in actions])\n",
        "    state = \"happy_end\" if up_count >= HAPPY_END_BOTDER else \"bad_end\"\n",
        "    prob = 1.0\n",
        "    return {state: prob}\n",
        "  else:\n",
        "    opposite = \"up\" if a == \"down\" else \"down\"\n",
        "    return {\n",
        "        next_state(s, a): MOVE_PROB,\n",
        "        next_state(s, opposite): 1 - MOVE_PROB\n",
        "    }"
      ],
      "metadata": {
        "id": "G32JZ5k4FsxP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. メインブロック\n",
        "目的: 関数 V をいくつかの状態に対して実行し、その結果を表示します。\n",
        "\n",
        "動作:\n",
        "\n",
        "* 初期状態 \"state\" における価値 V(\"state\") を計算して表示。\n",
        "* 行動 \"up\" を2回適用した状態 \"state_up_up\" における価値 V(\"state_up_up\") を計算して表示。\n",
        "* 行動 \"down\" を2回適用した状態 \"state_down_down\" における価値 V(\"state_down_down\") を計算して表示。\n"
      ],
      "metadata": {
        "id": "OKp9CNv9KNM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  print(V(\"state\"))\n",
        "  print(V(\"state_up_up\"))\n",
        "  print(V(\"state_down_down\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQVoIxOTGzOk",
        "outputId": "9c85eeca-80dd-4d5c-e288-78f325e9b61c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7880942034605892\n",
            "0.9068026334400001\n",
            "-0.96059601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code2-5 Value Iteration"
      ],
      "metadata": {
        "id": "MeXwdoNlgl6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Planner():\n",
        "\n",
        "  def __init__(self, env):\n",
        "    self.env = env\n",
        "    self.log = []\n",
        "\n",
        "  def initialize(self):\n",
        "    self.evn.reset()\n",
        "    self.log = []\n",
        "\n",
        "  def plan(self, gamma=0.9, threshold=0.0001):\n",
        "    raise Exception(\"Planner have to implements plan method.\")\n",
        "\n",
        "  def transitions_at(self, state, action):\n",
        "    transition_probs = self.env.transit_func(state, action)\n",
        "    for next_state in transition_probs:\n",
        "      prob = transition_probs[next_state]\n",
        "      reward, _ = self.env.reward_func(next_state)\n",
        "      yield prob, next_state, reward\n",
        "\n",
        "  def dict_to_grid(self, state_reward_dict):\n",
        "    grid = []\n",
        "    for i in range(self.env.row_length):\n",
        "      row = [0] * self.env.column_length\n",
        "      grid.append(row)\n",
        "    for s in state_reward_dict:\n",
        "      grid[s.row][s.column] = state_reward_dict[s]\n",
        "\n",
        "    return grid"
      ],
      "metadata": {
        "id": "Ex4dlR5miks-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code2-6"
      ],
      "metadata": {
        "id": "ZQANR1qfjPtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueIterationPlanner(Planner):\n",
        "\n",
        "  def __init__(self, env):\n",
        "     super().__init__(env)\n",
        "\n",
        "  def plan(self, gamma=0.9, threshold=0.0001):\n",
        "    self.initialize()\n",
        "    actions = self.env.actions\n",
        "    V = {}\n",
        "    for s in self.env.states:\n",
        "      V[s] = 0\n",
        "\n",
        "    while True:\n",
        "      delta = 0\n",
        "      self.log.append(self.dict_to_grid(V))\n",
        "      for s in V:\n",
        "        if not self.env.can_action_at(s):\n",
        "          continue\n",
        "        expected_rewards = []\n",
        "        for a in actions:\n",
        "          r = 0\n",
        "          for prob, next_state, reward in self.transitions_at(s, a):\n",
        "            r += prob * (reward + gamma * V[next_state])\n",
        "          expected_rewards.append(r)\n",
        "        max_reward = max(expected_rewards)\n",
        "        delta = max(delta, abs(max_reward - V[s]))\n",
        "        V[s] = max_reward\n",
        "      if delta < threshold:\n",
        "        break\n",
        "    V_grid = self.dict_to_grid(V)\n",
        "    return V_grid\n"
      ],
      "metadata": {
        "id": "RlUaXFjWkqct"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code2-7 localhost8888を立ち上げて起動させる(server)"
      ],
      "metadata": {
        "id": "NQPKi6-HwVHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code2-8"
      ],
      "metadata": {
        "id": "Oit12pmGxFkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyIterationPlanner(Planner):\n",
        "\n",
        "  def __init__(self, env):\n",
        "     super().__init__(env)\n",
        "     self.policy = {}\n",
        "\n",
        "  def initialize(self):\n",
        "    super().initialize()\n",
        "    self.policy = {}\n",
        "    actions = self.env.actions\n",
        "    states = self.env.next_states\n",
        "    for s in states:\n",
        "      self.policy[s] = {}\n",
        "      for a in actions:\n",
        "        self.policy[s][a] = 1 / len(actions)"
      ],
      "metadata": {
        "id": "lVk4ku6awfI3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code2-9"
      ],
      "metadata": {
        "id": "XmBmudwkxIgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_by_policy(self, gamma, threshold):\n",
        "  V = {}\n",
        "  for s in self.env.states:\n",
        "    V[s] = 0\n",
        "\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for s in V:\n",
        "      expected_rewards = []\n",
        "      for a in self.policy[s]:\n",
        "        action_prob = self.policy[s][a]\n",
        "        r = 0\n",
        "        for prob, next_state, reward in self.transitions_at(s, a):\n",
        "          r += action_prob * prob * \\\n",
        "          (reward + gamma * V[next_state])\n",
        "        expected_rewards.append(r)\n",
        "      value = sum(expected_rewards)\n",
        "      delta = max(delta, abs(value - V[s]))\n",
        "      V[s] = value\n",
        "    if delta < threshold:\n",
        "      break\n",
        "  return V"
      ],
      "metadata": {
        "id": "Ku2-KZt_yMkS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code2-10"
      ],
      "metadata": {
        "id": "TxpyNRsNz4aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plan(self, gamma=0.9, threshold=0.0001):\n",
        "  self.initialize()\n",
        "  states = self.env.states\n",
        "  actions = self.env.actions\n",
        "\n",
        "  def take_max_action(action_value_dict):\n",
        "    return max(action_value_dict, key=action_value_dict.get)\n",
        "\n",
        "  while True:\n",
        "    update_stable = True\n",
        "    V = self.estimate_by_policy(gamma, threshold)\n",
        "    self.log.append(self.dict_to_grid(V))\n",
        "\n",
        "    for s in states:\n",
        "      policy_action = take_max_action(self.policy[s])\n",
        "      action_rewards = {}\n",
        "      for a in actions:\n",
        "        r = 0\n",
        "        for prob, next_state, reward in self.transitions_at(s, a):\n",
        "          r += prob * (reward + gamma * V[next_state])\n",
        "        action_rewards[a] = r\n",
        "    best_action = take_max_action(action_rewards)\n",
        "    if policy_action != best_action:\n",
        "      update_stable = False\n",
        "    for a in self.policy[s]:\n",
        "      prob = 1 if a == best_action else 0\n",
        "      self.policy[s][a] = prob\n",
        "    if update_stable:\n",
        "      break\n",
        "  V_grid = self.dict_to_grid(V)\n",
        "  return V_grid"
      ],
      "metadata": {
        "id": "inQyMAO8z6RQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code2-11"
      ],
      "metadata": {
        "id": "RR_JaDMH1RGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment():\n",
        "\n",
        "  def __init__(self, grid, move_prob=0.8):\n",
        "    self.grid = grid\n",
        "    self.agent_state = State()\n",
        "    self.default_reward = -0.84\n",
        "    self.move_prob = move_prob\n",
        "    self.reset()"
      ],
      "metadata": {
        "id": "y7Qn-JKF1TeA"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}